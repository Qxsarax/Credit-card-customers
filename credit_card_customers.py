# -*- coding: utf-8 -*-
"""Credit_card_customers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KQIk7eN2tpuyxareJqDVDPCAz11hGXNn

## Cleaning the dataset with data preprocessing

Dataset analysis

---
"""

import pandas as pd
import numpy as np

base_url = "https://proai-datasets.s3.eu-west-3.amazonaws.com/credit_card_customers.csv"
df = pd.read_csv(base_url)
df.head()

df.shape

df.info()

df.describe()

"""I checked the null values ​​and replaced them with the mean.

---
"""

df.isna().sum()

for col in df.columns:
  if df[col].isnull().any():
    df[col].fillna(df[col].mean(), inplace=True)

df.head()

df.isna().sum()

"""I then transformed the **PRCFULLPAYMENT** column into a percentage

---

"""

percentage = (df['PRC_FULL_PAYMENT'] * 100).round(2)
df['PRC_FULL_PAYMENT'] = percentage
df.head()

"""I transformed the frequency of purchases, using a function, where the values ​​represented indicate:
- 1: frequent;
- 0: less frequent;

---

"""

def frequency(series):
  result = []
  for i in series:
    if i < 1:
      result.append(0)
    else:
      result.append(1)

  return result

df['BALANCE_FREQUENCY'] = frequency(df['BALANCE_FREQUENCY'])
df['PURCHASES_FREQUENCY'] = frequency(df['PURCHASES_FREQUENCY'])
df['ONEOFF_PURCHASES_FREQUENCY'] = frequency(df['ONEOFF_PURCHASES_FREQUENCY'])
df['PURCHASES_INSTALLMENTS_FREQUENCY'] = frequency(df['PURCHASES_INSTALLMENTS_FREQUENCY'])
df['CASH_ADVANCE_FREQUENCY'] = frequency(df['CASH_ADVANCE_FREQUENCY'])

df.head()

"""I rounded to two digits after the decimal point

---


"""

def round_values(df, columns):
  for col in columns:
    df[col] = df[col].round(2)

  return df

df = round_values(df, ['BALANCE', 'PAYMENTS', 'ONEOFF_PURCHASES', 'MINIMUM_PAYMENTS', 'CASH_ADVANCE'])
df.head()

"""Normalization


---


"""

from sklearn.preprocessing import MinMaxScaler

mms = MinMaxScaler()

x = df.drop(['CUST_ID'], axis=1)
x_norm = mms.fit_transform(x)

df = pd.DataFrame(x_norm, columns=x.columns)
df.head()

"""## Correlation matrix

I build a correlation matrix to more accurately select the variables


---
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(20, 12), dpi=80)

corr = df.corr()

hm = sns.heatmap(corr,
                 cbar=True,
                 square=True,
                 yticklabels=df.columns,
                 xticklabels=df.columns,
                 annot=True,
                 annot_kws={'size':12},
                 cmap= 'coolwarm')
plt.show()

"""## Clustering

### Customer Profiling Based on Spending and Payment Habits

---

I mainly focus on these variables:
- **BALANCE** : Amount of balance left on the account to make purchases;
- **PURCHASES** : Amount of purchases made from the account;
-**CASH_ADVANCE** : Cash advance given by the user;
- **MINIMUM_PAYMENTS** : Minimum amount of payments made by the user;

and I perform the normalization, then later the clustering will be applied only on the scaled data.

---
"""

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler

data = df[['BALANCE','PURCHASES', 'CASH_ADVANCE', 'MINIMUM_PAYMENTS']]


scaler = MinMaxScaler()
data_scaled = scaler.fit_transform(data)

k = 5

kmeans = KMeans(n_clusters=k, init="k-means++")
cluster = kmeans.fit_predict(data_scaled)

print(data)

RANDOM_STATE = 0

"""I created a function that applies the Elbow Method on the dataset to display the sum of the distances within the clusters versus the number of clusters

---
"""

x = data[['BALANCE','PURCHASES']].values

def elbow_method(x, max_cluster):

  # I used max_cluster to make the number of clusters to test dynamic, without fixing it to 10.

  sse = []

  for k in range(1, max_cluster + 1):
    kmeans = KMeans(n_clusters=k, n_init=10, random_state=RANDOM_STATE)
    kmeans.fit(x)
    sse.append(kmeans.inertia_)

  plt.figure(figsize=(12, 8))
  plt.plot(range(1, max_cluster + 1), sse, marker='o', color='b')

  font = {'family': 'serif',
        'color':  'black',
        'weight': 'normal',
        }

  plt.xlabel("Number of Clusters", fontsize=14, fontdict=font)
  plt.ylabel("Sum of Squared Distances", fontsize=14, fontdict=font)
  plt.title("The Elbow Method", fontsize=16, fontdict=font)

  plt.grid(True)
  plt.show()

"""The optimal number of clusters according to the Elbow Method is probably **2 or 3**. This means that dividing the data into 2 or 3 clusters maximizes the efficiency while reducing the SSD **without complicating the model**

---

"""

elbow_method(x, 10)

"""With this function I created a scatterplot to visualize the generated clusters. The colored dots are according to the clusters, the red dots are the cluster centers, and if requested it shows the SSD value between the dots and the centers

---
"""

def clusters(model, data, axlabels=None, print_ssd=False):

    y_pred = model.predict(data)
    fig, ax = plt.subplots(figsize=(12, 8))

    sns.scatterplot(x=data[:,0], y=data[:,1], hue=y_pred, s=100, palette='viridis', ax=ax)

    ax.scatter(model.cluster_centers_[:, 0], model.cluster_centers_[:, 1],
               c='red', s=200, alpha=0.5, marker='o', label='Centroids')

    if axlabels:
        ax.set_xlabel(axlabels[0], fontsize=16)
        ax.set_ylabel(axlabels[1], fontsize=16)

    if print_ssd:
      x_text = data[:, 0].max() - (data[:, 0].max() - data[:, 0].min()) * 0.1
      y_text = data[:, 1].min() + (data[:, 1].max() - data[:, 1].min()) * 0.1
      ax.text(x_text, y_text, f"SSD = {model.inertia_:.2f}", fontsize=14, color='black')

    ax.legend()
    plt.tight_layout()
    plt.show()

"""Model creation and training. Finally, cluster visualization

---

"""

kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = RANDOM_STATE).fit(x)
clusters(kmeans, x, axlabels=["BALANCE","PURCHASES"], print_ssd=True)

"""The graph shown has divided the customers into five clusters, based on two characteristics:

- **BALANCE** : amount of balance left on the account to make purchases;
- **PURCHASES** : Quantity of purchases made from the account;

The analysis shows that credit card customers are not all the same and can be divided into distinct groups based on their spending behavior and how they manage their debt. The red dots are the 'centers' of each group, showing the average position of all the dots in that group.

Each group has a characteristic profile:

- Cluster 0: customers with little balance left on the account and few purchases.
- Cluster 1: customers with a good balance between balance and purchases.
- Cluster 2: similar to the previous group, but they make even fewer purchases.
- Cluster 3: customers with little debt and many purchases.
- Cluster 4: customers with very little debt and very many purchases
---

"""

x = data[['BALANCE','CASH_ADVANCE']].values

elbow_method(x, 10)

kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = RANDOM_STATE).fit(x)
clusters(kmeans, x, axlabels=["BALANCE","CASH_ADVANCE"], print_ssd=True)

"""The graph represented has divided the customers into five clusters, based on two characteristics:

- **BALANCE** : amount of balance left in the account to make purchases;
- **CASH_ADVANCE** : Cash advance given by the user;

The analysis shows:
- Cluster 0: Represents customers with high balances and a moderate use of cash advances.
- Cluster 1: Represents customers with a good balance between balance and cash advances.
- Cluster 2: similar to the previous one, but with a slightly lower volume of cash advances.
- Cluster 3: Represents customers with a low residual debt and a moderate propensity to purchase.
- Cluster 4: Represents customers with very low balances and a very high volume of cash advance.

The SSD is made up of a low value (19.80) indicating that the points within each cluster are relatively close to each other.


---


"""

x = data[['PURCHASES','MINIMUM_PAYMENTS']].values

elbow_method(x, 10)

kmeans = KMeans(n_clusters = 5, init = 'k-means++', random_state = RANDOM_STATE).fit(x)
clusters(kmeans, x, axlabels=['PURCHASES','MINIMUM_PAYMENTS'], print_ssd=True)

"""The graph represented divided the customers into five clusters, based on two characteristics:

- **PURCHASES** : Quantity of purchases made from the account;
- **MINIMUM_PAYMENTS** : Minimum amount of payments made by the user;

The analysis revealed that:
- Cluster 0: Low volume of purchases and low minimum payments.
- Cluster 1: Moderate volume of purchases and low minimum payments.
- Cluster 2: High volume of purchases and high minimum payments.
- Cluster 3: Moderate volume of purchases and moderate minimum payments.
- Cluster 4: Low volume of purchases and moderate minimum payments.

The SSD value of 5.88 confirms that the clusters are well separated and homogeneous.


---



"""

x = df[['BALANCE','PURCHASES','MINIMUM_PAYMENTS']].values

from mpl_toolkits import mplot3d

def plot_clusters3d(model, data, axlabels=None):
  y_pred = model.predict(data)

  ax = plt.axes(projection ="3d")
  ax.scatter3D(data[:,0], data[:,1], data[:,2], edgecolors= "black", c=y_pred)
  ax.scatter3D(kmeans.cluster_centers_[:,0], kmeans.cluster_centers_[:,1], kmeans.cluster_centers_[:,2], color="red", s=100)

kmeans = KMeans(n_clusters=5, init = 'k-means++', random_state = RANDOM_STATE)
kmeans.fit(x)
print(f'Inertia: {kmeans.inertia_}')
plot_clusters3d(kmeans, x, axlabels=["BALANCE","PURCHASES"])

"""The position of a point in 3D space is determined by the values ​​of three variables: **BALANCE**, **PURCHASES**, and **MINIMUM_PAYMENTS**. For example, customers with high balances, many purchases, and high minimum payments will be positioned in a different area than customers with low balances, few purchases, and low minimum payments.


---


"""